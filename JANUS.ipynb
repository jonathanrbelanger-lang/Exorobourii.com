{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "authorship_tag": "ABX9TyP5nQcHjjzkWAyt0wBGPru/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jonathanrbelanger-lang/Exorobourii.com/blob/main/JANUS.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Project Janus: HTL/HTN Prototype"
      ],
      "metadata": {
        "id": "Up3c5Utpe3Fd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- CELL 1: SETUP, COMPILATION, AND PHYSICS FOUNDATION ---\n",
        "\n",
        "import torch\n",
        "import numpy as np\n",
        "from scipy.linalg import eigh\n",
        "import os\n",
        "import json\n",
        "from google.colab import drive\n",
        "import warnings\n",
        "\n",
        "# Suppress warnings related to complex number handling or minor library issues\n",
        "warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n",
        "\n",
        "# --- A. Environment Setup & Compilation Check ---\n",
        "\n",
        "# ELI12: First, we install necessary Python tools and the llama.cpp dependency.\n",
        "print(\"Installing Python dependencies...\")\n",
        "!pip install -q -U transformers accelerate sentencepiece llama-cpp-python\n",
        "# NOTE: llama-cpp-python is installed here to support later GGUF loading/inference\n",
        "\n",
        "\n",
        "# ELI12: Next, we download the entire 'llama.cpp' project from GitHub.\n",
        "print(\"Cloning llama.cpp repository...\")\n",
        "!git clone https://github.com/ggerganov/llama.cpp.git\n",
        "\n",
        "\n",
        "# ELI12: Compile the C++ tools using CMake.\n",
        "print(\"Configuring and compiling C++ tools...\")\n",
        "!cd llama.cpp && cmake -B build\n",
        "!cd llama.cpp && cmake --build build --config Release\n",
        "\n",
        "\n",
        "# ELI12: Install Python helper libraries required by llama.cpp scripts.\n",
        "print(\"Installing llama.cpp Python requirements...\")\n",
        "!cd llama.cpp && pip install -r requirements.txt\n",
        "\n",
        "\n",
        "# ELI12: CRITICAL ROBUSTNESS CHECK: Verify the key executable exists.\n",
        "QUANTIZE_PATH_CHECK = 'llama.cpp/build/bin/llama-quantize'\n",
        "\n",
        "if os.path.exists(QUANTIZE_PATH_CHECK):\n",
        "    print(f\"✅ Compilation successful. Quantize tool found at: {QUANTIZE_PATH_CHECK}\")\n",
        "else:\n",
        "    raise RuntimeError(\"❌ [FATAL SETUP ERROR] llama-quantize executable not found. Compilation failed.\")\n",
        "\n",
        "\n",
        "# ELI12: Finally, we import the Python libraries we'll use in this notebook.\n",
        "drive.mount('/content/drive')\n",
        "print(\"✅ Google Drive mounted.\")\n",
        "\n",
        "\n",
        "# --- B. Physics Utility Module Definition ---\n",
        "\n",
        "# Constants derived from Phase 1 validation\n",
        "E_STATE = -60.0\n",
        "E_TRANS = -2.00\n",
        "D_LOCAL = 16 # Local Hilbert Space Dimension\n",
        "\n",
        "def index_to_bits(k):\n",
        "    \"\"\"Converts index k (0-15) to (b3, b2, b1, b0).\"\"\"\n",
        "    b0 = k % 2\n",
        "    b1 = (k // 2) % 2\n",
        "    b2 = (k // 4) % 2\n",
        "    b3 = (k // 8) % 2\n",
        "    return np.array([b3, b2, b1, b0])\n",
        "\n",
        "def generate_OP_matrix():\n",
        "    \"\"\"Generates the 16x16 matrix for O_P = sum(sigma_z_i).\"\"\"\n",
        "    P = np.zeros((D_LOCAL, D_LOCAL), dtype=np.float64)\n",
        "    for k in range(D_LOCAL):\n",
        "        bits = index_to_bits(k)\n",
        "        diag_value = np.sum((-1)**bits)\n",
        "        P[k, k] = diag_value\n",
        "    return P\n",
        "\n",
        "def generate_OF_matrix():\n",
        "    \"\"\"Generates the 16x16 matrix for O'_F = sigma^x_0 * sigma^z_1 * sigma^z_2 * sigma^z_3.\"\"\"\n",
        "    F = np.zeros((D_LOCAL, D_LOCAL), dtype=np.float64)\n",
        "    for k in range(D_LOCAL):\n",
        "        bits = index_to_bits(k)\n",
        "        b3, b2, b1, b0 = bits\n",
        "\n",
        "        if b0 == 0:\n",
        "            k_prime = k + 1\n",
        "            sign = (-1)**(b1 + b2 + b3)\n",
        "            F[k, k_prime] = sign\n",
        "            F[k_prime, k] = sign\n",
        "    return F\n",
        "\n",
        "# --- CALCULATE AND STORE THE GROUND STATE VECTOR |psi_0> ---\n",
        "P_matrix = generate_OP_matrix()\n",
        "F_matrix = generate_OF_matrix()\n",
        "H_site_np = E_STATE * P_matrix + E_TRANS * F_matrix\n",
        "\n",
        "# Diagonalize to find the ground state (E_values sorted ascendingly)\n",
        "E_values, E_vectors = eigh(H_site_np)\n",
        "PSI_0_NP = E_vectors[:, 0] # The ground state vector (NumPy)\n",
        "\n",
        "# Convert to PyTorch Tensor (Complex type is essential for general quantum state representation)\n",
        "PSI_0_TORCH = torch.tensor(PSI_0_NP, dtype=torch.cfloat)\n",
        "\n",
        "# --- Utility Functions for ML Layer ---\n",
        "def normalize_input_to_state(v_patch):\n",
        "    \"\"\"Normalizes a 16-element input vector to represent a quantum state vector.\"\"\"\n",
        "    norm = torch.linalg.norm(v_patch)\n",
        "    if norm == 0:\n",
        "        return torch.ones_like(v_patch) / np.sqrt(16)\n",
        "    return v_patch / norm\n",
        "\n",
        "def calculate_projection_s(psi_in, psi_0):\n",
        "    \"\"\"Calculates the physics-derived feature s for the HTL: s = |<psi_0 | psi_in>|^2\"\"\"\n",
        "    # Ensure inputs are complex for correct inner product\n",
        "    overlap = torch.vdot(psi_0.to(psi_in.dtype), psi_in)\n",
        "    return torch.abs(overlap)**2\n",
        "\n",
        "print(\"\\n✅ Physics Foundation Established. $|\\psi_0\\rangle$ loaded as PyTorch Tensor.\")\n",
        "print(f\"Ground State Energy E0: {E_values[0]:.4f}\")"
      ],
      "metadata": {
        "id": "rLuKSKJA9d8d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# CELL 2: MNIST DATAMODULE WITH NOISE INJECTION (REVISED)\n",
        "# ============================================================================\n",
        "# This cell defines the data loading and augmentation pipeline.\n",
        "# No changes were required by the audit.\n",
        "# ============================================================================\n",
        "\n",
        "# ----------------------------------------------------------------------------\n",
        "# 1. IMPORT LIBRARIES\n",
        "# ----------------------------------------------------------------------------\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "from torchvision.datasets import MNIST\n",
        "from torchvision import transforms\n",
        "import pytorch_lightning as pl\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# ----------------------------------------------------------------------------\n",
        "# 2. CUSTOM TRANSFORM FOR PIXEL DROPOUT\n",
        "# ----------------------------------------------------------------------------\n",
        "class PixelDropout(torch.nn.Module):\n",
        "    \"\"\"\n",
        "    Custom transform to apply pixel dropout noise.\n",
        "    Sets a random fraction of pixels to 0.\n",
        "    \"\"\"\n",
        "    def __init__(self, epsilon: float):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            epsilon (float): The fraction of pixels to drop (e.g., 0.2 for 20%).\n",
        "                             Must be between 0.0 and 1.0.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        if not 0.0 <= epsilon <= 1.0:\n",
        "            raise ValueError(f\"Epsilon must be between 0 and 1, but got {epsilon}\")\n",
        "        self.epsilon = epsilon\n",
        "\n",
        "    def forward(self, img: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Applies the dropout to the input image tensor.\n",
        "        \"\"\"\n",
        "        if self.epsilon == 0.0:\n",
        "            return img\n",
        "\n",
        "        # Create a random mask of the same shape as the image\n",
        "        # bernoulli_(1 - epsilon) creates a mask where `1-epsilon` fraction of\n",
        "        # elements are 1 (kept) and `epsilon` are 0 (dropped).\n",
        "        mask = torch.empty_like(img).bernoulli_(1.0 - self.epsilon)\n",
        "\n",
        "        return img * mask\n",
        "\n",
        "    def __repr__(self) -> str:\n",
        "        return f\"{self.__class__.__name__}(epsilon={self.epsilon})\"\n",
        "\n",
        "# ----------------------------------------------------------------------------\n",
        "# 3. PYTORCH LIGHTNING DATAMODULE\n",
        "# ----------------------------------------------------------------------------\n",
        "class MNISTDataModule(pl.LightningDataModule):\n",
        "    \"\"\"\n",
        "    PyTorch Lightning DataModule for MNIST.\n",
        "    Handles downloading, splitting, transformations, and data loading.\n",
        "    Includes configurable pixel dropout noise.\n",
        "    \"\"\"\n",
        "    def __init__(self,\n",
        "                 data_dir: str = \"./data\",\n",
        "                 batch_size: int = 256,\n",
        "                 num_workers: int = 4,\n",
        "                 noise_level: float = 0.0):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            data_dir (str): Directory to save the MNIST data.\n",
        "            batch_size (int): Batch size for the data loaders.\n",
        "            num_workers (int): Number of subprocesses for data loading.\n",
        "            noise_level (float): Epsilon for PixelDropout in the training set.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.data_dir = data_dir\n",
        "        self.batch_size = batch_size\n",
        "        self.num_workers = num_workers\n",
        "        self.noise_level = noise_level\n",
        "        self.save_hyperparameters() # Saves args to self.hparams\n",
        "\n",
        "        # Define transformations\n",
        "        # 1. Resize to 32x32 to get 1024 dimensions\n",
        "        # 2. Convert to tensor\n",
        "        # 3. Normalize to [0, 1] range\n",
        "        # 4. Apply pixel dropout (for training set)\n",
        "        self.transform_train = transforms.Compose([\n",
        "            transforms.Resize((32, 32)),\n",
        "            transforms.ToTensor(),\n",
        "            PixelDropout(self.noise_level)\n",
        "        ])\n",
        "        self.transform_val_test = transforms.Compose([\n",
        "            transforms.Resize((32, 32)),\n",
        "            transforms.ToTensor()\n",
        "        ])\n",
        "\n",
        "    def prepare_data(self):\n",
        "        \"\"\"Download data if not present.\"\"\"\n",
        "        MNIST(self.data_dir, train=True, download=True)\n",
        "        MNIST(self.data_dir, train=False, download=True)\n",
        "        print(\"✅ MNIST data prepared.\")\n",
        "\n",
        "    def setup(self, stage: str = None):\n",
        "        \"\"\"\n",
        "        Assign train/val/test datasets for use in dataloaders.\n",
        "        This is called on every GPU in DDP.\n",
        "        \"\"\"\n",
        "        if stage == \"fit\" or stage is None:\n",
        "            mnist_full = MNIST(self.data_dir, train=True, transform=self.transform_train)\n",
        "            self.mnist_train, self.mnist_val = random_split(mnist_full, [55000, 5000])\n",
        "            # Override validation transform to be clean (no noise)\n",
        "            self.mnist_val.dataset.transform = self.transform_val_test\n",
        "\n",
        "        if stage == \"test\" or stage is None:\n",
        "            self.mnist_test = MNIST(self.data_dir, train=False, transform=self.transform_val_test)\n",
        "\n",
        "        print(\"✅ MNIST datasets assigned for train, validation, and test.\")\n",
        "\n",
        "    def train_dataloader(self):\n",
        "        return DataLoader(\n",
        "            self.mnist_train,\n",
        "            batch_size=self.batch_size,\n",
        "            num_workers=self.num_workers,\n",
        "            pin_memory=True, # A100 optimization\n",
        "            shuffle=True,\n",
        "            persistent_workers=True if self.num_workers > 0 else False\n",
        "        )\n",
        "\n",
        "    def val_dataloader(self):\n",
        "        return DataLoader(\n",
        "            self.mnist_val,\n",
        "            batch_size=self.batch_size,\n",
        "            num_workers=self.num_workers,\n",
        "            pin_memory=True, # A100 optimization\n",
        "            persistent_workers=True if self.num_workers > 0 else False\n",
        "        )\n",
        "\n",
        "    def test_dataloader(self):\n",
        "        return DataLoader(\n",
        "            self.mnist_test,\n",
        "            batch_size=self.batch_size,\n",
        "            num_workers=self.num_workers,\n",
        "            pin_memory=True, # A100 optimization\n",
        "            persistent_workers=True if self.num_workers > 0 else False\n",
        "        )\n",
        "\n",
        "# ----------------------------------------------------------------------------\n",
        "# 4. VERIFICATION AND DEBUG DUMP\n",
        "# ----------------------------------------------------------------------------\n",
        "def verify_datamodule(datamodule: MNISTDataModule, debug_dir: Path):\n",
        "    \"\"\"\n",
        "    Visualizes a sample batch to verify transformations and noise.\n",
        "    Saves the visualization as a debug artifact.\n",
        "    \"\"\"\n",
        "    print(\"\\n\" + \"=\" * 80)\n",
        "    print(\"Verifying DataModule and creating debug dump...\")\n",
        "    print(\"=\" * 80)\n",
        "\n",
        "    # Prepare and set up the datamodule\n",
        "    datamodule.prepare_data()\n",
        "    datamodule.setup(\"fit\")\n",
        "\n",
        "    # Get one batch from the training loader\n",
        "    train_loader = datamodule.train_dataloader()\n",
        "    images, labels = next(iter(train_loader))\n",
        "\n",
        "    print(f\"✅ Loaded one batch of data.\")\n",
        "    print(f\"  - Image batch shape: {images.shape}\")\n",
        "    print(f\"  - Label batch shape: {labels.shape}\")\n",
        "\n",
        "    # Check image dimensions\n",
        "    img_h, img_w = images.shape[2], images.shape[3]\n",
        "    if img_h != 32 or img_w != 32:\n",
        "        print(f\"❌ ERROR: Images are not 32x32. Found {img_h}x{img_w}.\", file=sys.stderr)\n",
        "        return\n",
        "\n",
        "    # Create a clean version of the same images for comparison\n",
        "    clean_transform = transforms.Compose([\n",
        "        transforms.Resize((32, 32)),\n",
        "        transforms.ToTensor()\n",
        "    ])\n",
        "    clean_dataset = MNIST(datamodule.data_dir, train=True, transform=clean_transform)\n",
        "    clean_loader = DataLoader(clean_dataset, batch_size=datamodule.batch_size)\n",
        "    clean_images, _ = next(iter(clean_loader))\n",
        "\n",
        "    # --- Visualization ---\n",
        "    fig, axes = plt.subplots(4, 4, figsize=(10, 10))\n",
        "    fig.suptitle(f\"DataModule Verification\\nTrain Noise (ε={datamodule.noise_level}) vs. Clean\", fontsize=16)\n",
        "\n",
        "    for i in range(8):\n",
        "        # Plot clean image\n",
        "        ax_clean = axes[i // 2, (i % 2) * 2]\n",
        "        ax_clean.imshow(clean_images[i].squeeze(), cmap=\"gray\")\n",
        "        ax_clean.set_title(f\"Clean (Label: {labels[i]})\")\n",
        "        ax_clean.axis(\"off\")\n",
        "\n",
        "        # Plot noisy image from the actual train loader\n",
        "        ax_noisy = axes[i // 2, (i % 2) * 2 + 1]\n",
        "        ax_noisy.imshow(images[i].squeeze(), cmap=\"gray\")\n",
        "        ax_noisy.set_title(f\"Noisy (ε={datamodule.noise_level})\")\n",
        "        ax_noisy.axis(\"off\")\n",
        "\n",
        "    plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
        "\n",
        "    # Save the plot to the debug directory\n",
        "    dump_path = debug_dir / \"datamodule_verification.png\"\n",
        "    plt.savefig(dump_path)\n",
        "    print(f\"\\n✅ Verification plot saved to: {dump_path}\")\n",
        "    plt.show()\n",
        "    print(\"=\" * 80)\n",
        "    print(\"DataModule verification complete.\")\n",
        "    print(\"=\" * 80)\n",
        "\n",
        "\n",
        "# --- Main Execution ---\n",
        "# Instantiate the DataModule with a moderate noise level for verification\n",
        "# We will create new instances later for actual training runs.\n",
        "data_module = MNISTDataModule(\n",
        "    batch_size=256,\n",
        "    num_workers=os.cpu_count() // 2, # Use a safe number of workers\n",
        "    noise_level=0.3 # 30% pixel dropout for this test\n",
        ")\n",
        "\n",
        "# Run the verification and generate the debug dump\n",
        "verify_datamodule(data_module, PROJECT_CONFIG[\"debug_dir\"])"
      ],
      "metadata": {
        "id": "EmTam0qSK2eO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# CELL 3: BASE LIGHTNINGMODULE FOR MODEL ARCHITECTURE (REVISED)\n",
        "# ============================================================================\n",
        "# This cell defines the shared architecture and training logic for both\n",
        "# the HTL and Dense models to ensure a fair comparison.\n",
        "# ============================================================================\n",
        "\n",
        "# ----------------------------------------------------------------------------\n",
        "# 1. IMPORT LIBRARIES\n",
        "# ----------------------------------------------------------------------------\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import pytorch_lightning as pl\n",
        "from torch.optim import AdamW\n",
        "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
        "from torchinfo import summary\n",
        "from typing import Dict, Any\n",
        "\n",
        "# Import our custom physics module\n",
        "import physics_utils\n",
        "\n",
        "# ----------------------------------------------------------------------------\n",
        "# 2. DEFINE THE BASE LIGHTNINGMODULE\n",
        "# ----------------------------------------------------------------------------\n",
        "class BaseLitModel(pl.LightningModule):\n",
        "    \"\"\"\n",
        "    A base LightningModule that defines the shared architecture and logic.\n",
        "    This class is designed to be inherited by specific model implementations\n",
        "    (e.g., HTLModel, DenseModel).\n",
        "    \"\"\"\n",
        "    def __init__(self,\n",
        "                 learning_rate: float = 1e-3,\n",
        "                 weight_decay: float = 1e-4,\n",
        "                 total_steps: int = 10000):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            learning_rate (float): The peak learning rate for the optimizer.\n",
        "            weight_decay (float): The weight decay for the AdamW optimizer.\n",
        "            total_steps (int): Total training steps, used for the LR scheduler.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        # Save hyperparameters for logging and checkpointing\n",
        "        self.save_hyperparameters(\"learning_rate\", \"weight_decay\")\n",
        "        self.total_steps = total_steps\n",
        "\n",
        "        # --- Architecture Definition ---\n",
        "        # This architecture is designed to meet the ~132K parameter budget.\n",
        "\n",
        "        # 1. Feature Extractor (placeholder to be defined in child classes)\n",
        "        # This is the ONLY part that will differ between the HTL and Dense models.\n",
        "        self.feature_extractor = None # Must be set by child class\n",
        "\n",
        "        # 2. Classifier Head (IDENTICAL for both models)\n",
        "        # This ensures a fair comparison of the feature extractors.\n",
        "        # The input dimension (128) is chosen to balance the parameter counts.\n",
        "        self.classifier_head = nn.Sequential(\n",
        "            nn.Linear(128, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(128, 10) # 10 classes for MNIST\n",
        "        )\n",
        "\n",
        "        # --- Loss Function ---\n",
        "        self.criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Defines the forward pass of the model.\n",
        "\n",
        "        Args:\n",
        "            x (torch.Tensor): Input tensor of shape (N, 1, 32, 32)\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: Output logits of shape (N, 10)\n",
        "        \"\"\"\n",
        "        # Flatten the input image: (N, 1, 32, 32) -> (N, 1024)\n",
        "        x = x.view(x.size(0), -1)\n",
        "\n",
        "        # Pass through the specific feature extractor (HTL or Dense)\n",
        "        features = self.feature_extractor(x)\n",
        "\n",
        "        # Pass features through the common classifier head\n",
        "        logits = self.classifier_head(features)\n",
        "\n",
        "        return logits\n",
        "\n",
        "    def _shared_step(self, batch: tuple, batch_idx: int) -> Dict[str, Any]:\n",
        "        \"\"\"\n",
        "        Common logic for training, validation, and test steps.\n",
        "        \"\"\"\n",
        "        x, y = batch\n",
        "        logits = self(x)\n",
        "        loss = self.criterion(logits, y)\n",
        "\n",
        "        preds = torch.argmax(logits, dim=1)\n",
        "        acc = (preds == y).float().mean()\n",
        "\n",
        "        return {\"loss\": loss, \"acc\": acc}\n",
        "\n",
        "    def training_step(self, batch: tuple, batch_idx: int) -> torch.Tensor:\n",
        "        result = self._shared_step(batch, batch_idx)\n",
        "        self.log('train_loss', result['loss'], on_step=True, on_epoch=True, prog_bar=True, logger=True)\n",
        "        self.log('train_acc', result['acc'], on_step=True, on_epoch=True, prog_bar=True, logger=True)\n",
        "        return result['loss']\n",
        "\n",
        "    def validation_step(self, batch: tuple, batch_idx: int):\n",
        "        result = self._shared_step(batch, batch_idx)\n",
        "        self.log('val_loss', result['loss'], on_epoch=True, prog_bar=True, logger=True)\n",
        "        self.log('val_acc', result['acc'], on_epoch=True, prog_bar=True, logger=True)\n",
        "\n",
        "    def test_step(self, batch: tuple, batch_idx: int):\n",
        "        result = self._shared_step(batch, batch_idx)\n",
        "        self.log('test_loss', result['loss'], on_epoch=True, logger=True)\n",
        "        self.log('test_acc', result['acc'], on_epoch=True, logger=True)\n",
        "\n",
        "    def configure_optimizers(self) -> Dict[str, Any]:\n",
        "        \"\"\"\n",
        "        Configures the optimizer and learning rate scheduler.\n",
        "        \"\"\"\n",
        "        optimizer = AdamW(self.parameters(), lr=self.hparams.learning_rate, weight_decay=self.hparams.weight_decay)\n",
        "\n",
        "        scheduler = {\n",
        "            \"scheduler\": CosineAnnealingLR(optimizer, T_max=self.total_steps, eta_min=1e-6),\n",
        "            \"interval\": \"step\",\n",
        "            \"frequency\": 1,\n",
        "        }\n",
        "\n",
        "        return {\"optimizer\": optimizer, \"lr_scheduler\": scheduler}\n",
        "\n",
        "    def model_summary(self, batch_size: int = 64):\n",
        "        \"\"\"\n",
        "        Prints a detailed summary of the model architecture and parameters.\n",
        "        \"\"\"\n",
        "        # The input size is (batch_size, channels, height, width)\n",
        "        input_size = (batch_size, 1, 32, 32)\n",
        "        print(\"\\n\" + \"=\" * 80)\n",
        "        print(f\"MODEL SUMMARY: {self.__class__.__name__}\")\n",
        "        print(\"=\" * 80)\n",
        "        summary(self, input_size=input_size, col_names=[\"input_size\", \"output_size\", \"num_params\", \"mult_adds\"])\n",
        "        print(\"=\" * 80)\n",
        "\n",
        "# ----------------------------------------------------------------------------\n",
        "# 3. DEFINE THE SPECIFIC MODEL IMPLEMENTATIONS\n",
        "# ----------------------------------------------------------------------------\n",
        "\n",
        "# --- HTL Model ---\n",
        "class HTLModel(BaseLitModel):\n",
        "    \"\"\"\n",
        "    The full model using the Hex-Tensor Layer (HTL) as the feature extractor.\n",
        "    \"\"\"\n",
        "    def __init__(self, psi_0: np.ndarray, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "\n",
        "        # The feature extractor is our custom HTL.\n",
        "        # Input: 1024, Output: 128\n",
        "        self.feature_extractor = physics_utils.HexTensorLayer(psi_0, output_dim=128)\n",
        "\n",
        "# --- Dense Baseline Model ---\n",
        "class DenseModel(BaseLitModel):\n",
        "    \"\"\"\n",
        "    The baseline model using a standard Dense layer as the feature extractor.\n",
        "    Parameter count is matched to the HTL model for a fair comparison.\n",
        "    \"\"\"\n",
        "    def __init__(self, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "\n",
        "        # The feature extractor is a standard Dense layer.\n",
        "        # Input: 1024, Output: 128\n",
        "        self.feature_extractor = physics_utils.StandardDenseLayer(\n",
        "            input_dim=PROJECT_CONSTANTS[\"HILBERT_DIM\"],\n",
        "            output_dim=128\n",
        "        )"
      ],
      "metadata": {
        "id": "BgRjaIWgLgKg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# CELL 4: PIPELINE VERIFICATION & SANITY CHECK (REVISED)\n",
        "# ============================================================================\n",
        "# This cell prepares all components for the main experiment, verifies their\n",
        "# correctness, and runs a brief \"smoke test\" to ensure the pipeline is functional.\n",
        "# ============================================================================\n",
        "\n",
        "# ----------------------------------------------------------------------------\n",
        "# 1. IMPORT LIBRARIES\n",
        "# ----------------------------------------------------------------------------\n",
        "import torch\n",
        "import pytorch_lightning as pl\n",
        "from pytorch_lightning.callbacks import ModelCheckpoint, LearningRateMonitor, EarlyStopping\n",
        "from pytorch_lightning.loggers import TensorBoardLogger\n",
        "from pytorch_lightning.profilers import PyTorchProfiler\n",
        "from typing import Type\n",
        "import numpy as np\n",
        "\n",
        "# Import our project modules\n",
        "import physics_utils\n",
        "# The model classes are available from the execution of the revised Cell 3.\n",
        "\n",
        "# ----------------------------------------------------------------------------\n",
        "# 2. EXPERIMENT RUNNER FUNCTION\n",
        "# ----------------------------------------------------------------------------\n",
        "def run_experiment(\n",
        "    model_class: Type[BaseLitModel],\n",
        "    config: dict,\n",
        "    project_dirs: dict,\n",
        "    psi_0: np.ndarray = None,\n",
        "    smoke_test_steps: int = None\n",
        "):\n",
        "    \"\"\"\n",
        "    Executes a full training and testing pipeline for a given model and config.\n",
        "\n",
        "    Args:\n",
        "        model_class (Type[BaseLitModel]): The model class to train (HTLModel or DenseModel).\n",
        "        config (dict): A dictionary of training hyperparameters.\n",
        "        project_dirs (dict): A dictionary of project directories.\n",
        "        psi_0 (np.ndarray, optional): The ground state vector, required for HTLModel.\n",
        "        smoke_test_steps (int, optional): If provided, limits training to this many steps.\n",
        "    \"\"\"\n",
        "    model_name = model_class.__name__\n",
        "    print(\"\\n\" + \"=\" * 80)\n",
        "    print(f\"CONFIGURING EXPERIMENT: {model_name} with Noise ε = {config['noise_level']}\")\n",
        "    print(\"=\" * 80)\n",
        "\n",
        "    # --- 1. Setup DataModule ---\n",
        "    datamodule = MNISTDataModule(\n",
        "        batch_size=config[\"batch_size\"],\n",
        "        num_workers=config[\"num_workers\"],\n",
        "        noise_level=config[\"noise_level\"]\n",
        "    )\n",
        "    datamodule.setup('fit')\n",
        "\n",
        "    # --- 2. Calculate Total Steps for LR Scheduler ---\n",
        "    steps_per_epoch = len(datamodule.train_dataloader())\n",
        "    if smoke_test_steps:\n",
        "        # For a short smoke test, don't base scheduler on full run\n",
        "        total_steps = smoke_test_steps\n",
        "    else:\n",
        "        total_steps = steps_per_epoch * config[\"max_epochs\"]\n",
        "    print(f\"Training details: {steps_per_epoch} steps/epoch, {total_steps} total steps planned.\")\n",
        "\n",
        "    # --- 3. Initialize Model ---\n",
        "    model_args = {\n",
        "        \"learning_rate\": config[\"learning_rate\"],\n",
        "        \"weight_decay\": config[\"weight_decay\"],\n",
        "        \"total_steps\": total_steps\n",
        "    }\n",
        "    if model_class == HTLModel:\n",
        "        if psi_0 is None:\n",
        "            raise ValueError(\"psi_0 must be provided for HTLModel\")\n",
        "        model = HTLModel(psi_0=psi_0, **model_args)\n",
        "    else:\n",
        "        model = DenseModel(**model_args)\n",
        "\n",
        "    # REVISION: torch.compile is now applied here, to the model instance.\n",
        "    try:\n",
        "        print(f\"Compiling {model_name} with torch.compile()...\")\n",
        "        model = torch.compile(model, mode=\"max-autotune\")\n",
        "        print(f\"✅ {model_name} compiled successfully.\")\n",
        "    except Exception as e:\n",
        "        print(f\"⚠️ WARNING: torch.compile() failed: {e}. Using uncompiled model.\")\n",
        "\n",
        "    # --- 4. Configure Callbacks & Logger ---\n",
        "    logger = TensorBoardLogger(project_dirs[\"log_dir\"], name=f\"{model_name}_noise_{config['noise_level']:.1f}\")\n",
        "    profiler = PyTorchProfiler(\n",
        "        dirpath=project_dirs[\"debug_dir\"], filename=f\"{model_name}_profile\",\n",
        "        schedule=torch.profiler.schedule(wait=1, warmup=1, active=5, repeat=1),\n",
        "        on_trace_ready=torch.profiler.tensorboard_trace_handler(logger.log_dir)\n",
        "    )\n",
        "    checkpoint_callback = ModelCheckpoint(\n",
        "        dirpath=project_dirs[\"checkpoint_dir\"], monitor=\"val_acc\", mode=\"max\", save_top_k=1, verbose=False\n",
        "    )\n",
        "    lr_monitor = LearningRateMonitor(logging_interval='step')\n",
        "    early_stopping = EarlyStopping(monitor=\"val_loss\", patience=config[\"early_stopping_patience\"], mode=\"min\", verbose=False)\n",
        "\n",
        "    # --- 5. Configure Trainer ---\n",
        "    trainer = pl.Trainer(\n",
        "        accelerator=\"gpu\", devices=1, max_epochs=config[\"max_epochs\"],\n",
        "        logger=logger, profiler=profiler,\n",
        "        callbacks=[checkpoint_callback, lr_monitor, early_stopping],\n",
        "        precision=\"16-mixed\", benchmark=True, log_every_n_steps=20,\n",
        "        limit_train_batches=smoke_test_steps if smoke_test_steps else 1.0\n",
        "    )\n",
        "\n",
        "    # --- 6. Run Training & Testing ---\n",
        "    print(f\"\\n--- Starting run for {model_name} ---\")\n",
        "    with torch.cuda.nvtx.range(\"trainer.fit\"):\n",
        "        trainer.fit(model, datamodule=datamodule)\n",
        "\n",
        "    with torch.cuda.nvtx.range(\"trainer.test\"):\n",
        "        test_results = trainer.test(ckpt_path=\"best\", datamodule=datamodule, verbose=False)\n",
        "    print(f\"--- Run finished for {model_name} ---\")\n",
        "\n",
        "    print(f\"  - Best Checkpoint: {checkpoint_callback.best_model_path}\")\n",
        "    print(f\"  - Final Test Accuracy:   {test_results[0]['test_acc']:.4f}\")\n",
        "\n",
        "    return test_results[0]\n",
        "\n",
        "# ----------------------------------------------------------------------------\n",
        "# 4. MAIN EXECUTION BLOCK: SETUP, VERIFICATION, AND SMOKE TEST\n",
        "# ----------------------------------------------------------------------------\n",
        "if __name__ == '__main__':\n",
        "    # --- 1. Initialize Physics Foundation (Single Source of Truth) ---\n",
        "    psi_0, E_0 = physics_utils.initialize_physics_foundation(\n",
        "        e_state=PROJECT_CONSTANTS[\"E_STATE\"],\n",
        "        e_trans=PROJECT_CONSTANTS[\"E_TRANS\"]\n",
        "    )\n",
        "    psi_0_path = PROJECT_CONFIG[\"debug_dir\"] / \"psi_0_ground_state.npy\"\n",
        "    np.save(psi_0_path, psi_0)\n",
        "    print(f\"\\n✅ Ground state vector created and saved to: {psi_0_path}\")\n",
        "\n",
        "    # --- 2. Verify Model Architectures and Parameter Counts ---\n",
        "    # REVISION: This logic is moved from Cell 3's __main__ block.\n",
        "    print(\"\\n\" + \"=\" * 80)\n",
        "    print(\"VERIFYING MODEL PARAMETER COUNTS\")\n",
        "    print(\"=\" * 80)\n",
        "\n",
        "    # Instantiate models for verification\n",
        "    htl_model_verify = HTLModel(psi_0=psi_0, total_steps=1)\n",
        "    dense_model_verify = DenseModel(total_steps=1)\n",
        "\n",
        "    htl_params = sum(p.numel() for p in htl_model_verify.parameters() if p.requires_grad)\n",
        "    dense_params = sum(p.numel() for p in dense_model_verify.parameters() if p.requires_grad)\n",
        "\n",
        "    print(f\"Target Parameter Count: ~132,000\")\n",
        "    print(f\"  - HTL Model Total Parameters:   {htl_params:,}\")\n",
        "    print(f\"  - Dense Model Total Parameters: {dense_params:,}\")\n",
        "\n",
        "    param_diff_percent = abs(htl_params - dense_params) / dense_params * 100\n",
        "    if param_diff_percent < 1.0:\n",
        "        print(\"✅ SUCCESS: Parameter counts are matched within a 1% tolerance.\")\n",
        "    else:\n",
        "        print(f\"❌ FAILURE: Parameter counts do not match ({param_diff_percent:.2f}% diff).\", file=sys.stderr)\n",
        "\n",
        "    htl_model_verify.model_summary(batch_size=64)\n",
        "\n",
        "    # --- 3. Run a Brief Smoke Test ---\n",
        "    print(\"\\n\" + \"=\" * 80)\n",
        "    print(\"PERFORMING A QUICK SMOKE TEST ON THE HTL MODEL\")\n",
        "    print(\"=\" * 80)\n",
        "\n",
        "    SMOKE_TEST_CONFIG = {\n",
        "        \"model_type\": \"HTL\", \"noise_level\": 0.3, \"learning_rate\": 1e-3,\n",
        "        \"weight_decay\": 1e-4, \"batch_size\": 512, \"max_epochs\": 1,\n",
        "        \"num_workers\": os.cpu_count() // 2, \"early_stopping_patience\": 3,\n",
        "    }\n",
        "\n",
        "    run_experiment(\n",
        "        model_class=HTLModel,\n",
        "        config=SMOKE_TEST_CONFIG,\n",
        "        project_dirs=PROJECT_CONFIG,\n",
        "        psi_0=psi_0,\n",
        "        smoke_test_steps=50  # Limit to 50 training batches\n",
        "    )\n",
        "\n",
        "    print(\"\\n\" + \"*\" * 80)\n",
        "    print(\"PIPELINE VERIFIED: System is ready for the full experimental suite.\")\n",
        "    print(\"The `psi_0` object is now available for the next cell.\")\n",
        "    print(\"*\" * 80)"
      ],
      "metadata": {
        "id": "YXl1xJC6LiUZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# CELL 5: PHASE 1B - FULL EXPERIMENTAL SUITE (REVISED)\n",
        "# ============================================================================\n",
        "# This cell executes the complete, statistically robust Phase 1b experimental\n",
        "# protocol. It replaces the original Cell 5.\n",
        "# WARNING: This cell will run for several hours.\n",
        "# ============================================================================\n",
        "\n",
        "# ----------------------------------------------------------------------------\n",
        "# 1. IMPORT LIBRARIES\n",
        "# ----------------------------------------------------------------------------\n",
        "import pandas as pd\n",
        "import time\n",
        "from datetime import timedelta\n",
        "import pytorch_lightning as pl\n",
        "from pytorch_lightning.loggers import TensorBoardLogger\n",
        "from pytorch_lightning.callbacks import EarlyStopping, ModelCheckpoint\n",
        "\n",
        "# ----------------------------------------------------------------------------\n",
        "# 2. PREREQUISITE CHECK\n",
        "# ----------------------------------------------------------------------------\n",
        "if 'psi_0' not in locals() or psi_0 is None:\n",
        "    raise NameError(\"The `psi_0` ground state vector is not defined. Please run the verification cell (Cell 4) before this one.\")\n",
        "else:\n",
        "    print(\"✅ Prerequisite check passed: `psi_0` ground state is available.\")\n",
        "\n",
        "# ----------------------------------------------------------------------------\n",
        "# 3. DEFINE PHASE 1B EXPERIMENTAL PROTOCOL\n",
        "# ----------------------------------------------------------------------------\n",
        "# This grid defines the three arms of our experiment.\n",
        "MODEL_CONFIGS = [\n",
        "    {\n",
        "        'name': 'HTL',\n",
        "        'model_class': HTLModel,\n",
        "        'train_noise': 0.3,  # HTL is trained on noisy data\n",
        "    },\n",
        "    {\n",
        "        'name': 'Dense',\n",
        "        'model_class': DenseModel,\n",
        "        'train_noise': 0.0,   # Standard baseline is trained on clean data\n",
        "    },\n",
        "    {\n",
        "        'name': 'DenseAug',\n",
        "        'model_class': DenseModel,  # Same architecture as Dense...\n",
        "        'train_noise': 0.3,  # ...but trained on noisy data (the critical control)\n",
        "    }\n",
        "]\n",
        "\n",
        "TEST_NOISE_LEVELS = [0.0, 0.1, 0.2, 0.3, 0.4, 0.5]\n",
        "RANDOM_SEEDS = [42, 123, 456]  # Use 3 seeds for statistical validity\n",
        "\n",
        "# Base configuration for all runs\n",
        "BASE_TRAIN_CONFIG = {\n",
        "    \"learning_rate\": 1e-3, \"weight_decay\": 1e-4, \"batch_size\": 512,\n",
        "    \"max_epochs\": 20, \"num_workers\": os.cpu_count() // 2,\n",
        "    \"early_stopping_patience\": 3,\n",
        "}\n",
        "\n",
        "# ----------------------------------------------------------------------------\n",
        "# 4. AUTOMATED EXPERIMENT EXECUTION LOOP\n",
        "# ----------------------------------------------------------------------------\n",
        "all_results = []\n",
        "total_runs = len(RANDOM_SEEDS) * len(MODEL_CONFIGS)\n",
        "current_run = 0\n",
        "total_start_time = time.time()\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"STARTING PROJECT JANUS: PHASE 1B EXPERIMENTAL SUITE\")\n",
        "print(f\"Total models to train: {total_runs} ({len(MODEL_CONFIGS)} configs x {len(RANDOM_SEEDS)} seeds)\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "for seed in RANDOM_SEEDS:\n",
        "    for model_config in MODEL_CONFIGS:\n",
        "        current_run += 1\n",
        "        run_start_time = time.time()\n",
        "\n",
        "        # --- Set Seed for this Run ---\n",
        "        pl.seed_everything(seed, workers=True)\n",
        "\n",
        "        model_name = model_config['name']\n",
        "        train_noise = model_config['train_noise']\n",
        "\n",
        "        print(\"\\n\" + \"*\" * 80)\n",
        "        print(f\"RUN {current_run}/{total_runs}: Training model '{model_name}' with seed={seed}, train_noise={train_noise}\")\n",
        "        print(\"*\" * 80)\n",
        "\n",
        "        # --- 1. Initialize DataModule for TRAINING ---\n",
        "        train_datamodule = MNISTDataModule(\n",
        "            batch_size=BASE_TRAIN_CONFIG[\"batch_size\"],\n",
        "            num_workers=BASE_TRAIN_CONFIG[\"num_workers\"],\n",
        "            noise_level=train_noise\n",
        "        )\n",
        "        train_datamodule.setup('fit')\n",
        "\n",
        "        # --- 2. Initialize Model ---\n",
        "        steps_per_epoch = len(train_datamodule.train_dataloader())\n",
        "        total_steps = steps_per_epoch * BASE_TRAIN_CONFIG[\"max_epochs\"]\n",
        "\n",
        "        model_args = {\n",
        "            \"learning_rate\": BASE_TRAIN_CONFIG[\"learning_rate\"],\n",
        "            \"weight_decay\": BASE_TRAIN_CONFIG[\"weight_decay\"],\n",
        "            \"total_steps\": total_steps\n",
        "        }\n",
        "        ModelClass = model_config['model_class']\n",
        "        if ModelClass == HTLModel:\n",
        "            model = HTLModel(psi_0=psi_0, **model_args)\n",
        "        else:\n",
        "            model = DenseModel(**model_args)\n",
        "\n",
        "        # Apply torch.compile\n",
        "        try:\n",
        "            model = torch.compile(model, mode=\"max-autotune\")\n",
        "        except Exception:\n",
        "            pass # Failsafe for non-compatible versions\n",
        "\n",
        "        # --- 3. Configure Trainer and Train the Model ---\n",
        "        logger = TensorBoardLogger(PROJECT_CONFIG[\"log_dir\"], name=f\"{model_name}_seed{seed}_train_noise{train_noise}\")\n",
        "        checkpoint_callback = ModelCheckpoint(monitor=\"val_acc\", mode=\"max\")\n",
        "\n",
        "        trainer = pl.Trainer(\n",
        "            accelerator=\"gpu\", devices=1, max_epochs=BASE_TRAIN_CONFIG[\"max_epochs\"],\n",
        "            logger=logger, precision=\"16-mixed\", benchmark=True, deterministic=True,\n",
        "            callbacks=[EarlyStopping(monitor=\"val_loss\", patience=BASE_TRAIN_CONFIG[\"early_stopping_patience\"]), checkpoint_callback],\n",
        "            enable_progress_bar=True, log_every_n_steps=50\n",
        "        )\n",
        "\n",
        "        with torch.cuda.nvtx.range(f\"fit_{model_name}_seed{seed}\"):\n",
        "            trainer.fit(model, datamodule=train_datamodule)\n",
        "\n",
        "        # --- 4. Test the SINGLE Trained Model on MULTIPLE Noise Levels ---\n",
        "        print(f\"\\n--- Testing '{model_name}' (seed={seed}) across noise spectrum ---\")\n",
        "        best_model_path = checkpoint_callback.best_model_path\n",
        "\n",
        "        for test_noise in TEST_NOISE_LEVELS:\n",
        "            # Create a new datamodule for each test noise level\n",
        "            test_datamodule = MNISTDataModule(\n",
        "                batch_size=BASE_TRAIN_CONFIG[\"batch_size\"],\n",
        "                num_workers=BASE_TRAIN_CONFIG[\"num_workers\"],\n",
        "                noise_level=0.0 # Test data itself is clean before flattening\n",
        "            )\n",
        "\n",
        "            # Manually create a test loader with noise transform for evaluation\n",
        "            test_dataset = MNIST(test_datamodule.data_dir, train=False, transform=transforms.Compose([\n",
        "                transforms.Resize((32, 32)),\n",
        "                transforms.ToTensor(),\n",
        "                PixelDropout(test_noise)\n",
        "            ]))\n",
        "            test_loader = DataLoader(test_dataset, batch_size=BASE_TRAIN_CONFIG[\"batch_size\"], num_workers=BASE_TRAIN_CONFIG[\"num_workers\"])\n",
        "\n",
        "            with torch.cuda.nvtx.range(f\"test_{model_name}_seed{seed}_test_noise{test_noise}\"):\n",
        "                test_results = trainer.test(model, dataloaders=test_loader, ckpt_path=best_model_path, verbose=False)\n",
        "\n",
        "            test_acc = test_results[0]['test_acc']\n",
        "\n",
        "            # Store the comprehensive result\n",
        "            all_results.append({\n",
        "                'model': model_name,\n",
        "                'seed': seed,\n",
        "                'train_noise': train_noise,\n",
        "                'test_noise': test_noise,\n",
        "                'test_accuracy': test_acc\n",
        "            })\n",
        "            print(f\"  - Test Accuracy at noise ε={test_noise:.1f}: {test_acc:.4f}\")\n",
        "\n",
        "        run_elapsed_time = time.time() - run_start_time\n",
        "        print(f\"--- Run {current_run} finished in {timedelta(seconds=run_elapsed_time)} ---\")\n",
        "\n",
        "total_elapsed_time = time.time() - total_start_time\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"PHASE 1B EXPERIMENTAL SUITE COMPLETE\")\n",
        "print(f\"Total execution time: {timedelta(seconds=total_elapsed_time)}\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# ----------------------------------------------------------------------------\n",
        "# 5. SAVE RAW RESULTS\n",
        "# ----------------------------------------------------------------------------\n",
        "results_df = pd.DataFrame(all_results)\n",
        "results_csv_path = PROJECT_CONFIG[\"run_dir\"] / \"phase1b_full_results.csv\"\n",
        "results_df.to_csv(results_csv_path, index=False)\n",
        "\n",
        "print(\"\\n--- Raw Results Summary ---\")\n",
        "print(results_df.to_string())\n",
        "print(f\"\\n✅ Full raw results saved to: {results_csv_path}\")\n",
        "print(\"\\nProceed to the next cells for visualization and statistical analysis.\")"
      ],
      "metadata": {
        "id": "6PmmSE1QLqvV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# CELL 6: PHASE 1B - RESULTS VISUALIZATION\n",
        "# ============================================================================\n",
        "# This cell aggregates the raw experimental data and generates the final,\n",
        "# publication-quality comparative plot with statistical error bars.\n",
        "# ============================================================================\n",
        "\n",
        "# ----------------------------------------------------------------------------\n",
        "# 1. IMPORT LIBRARIES\n",
        "# ----------------------------------------------------------------------------\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# ----------------------------------------------------------------------------\n",
        "# 2. PREREQUISITE CHECK\n",
        "# ----------------------------------------------------------------------------\n",
        "# Ensure the raw results from Cell 5 are available.\n",
        "if 'results_df' not in locals() or not isinstance(results_df, pd.DataFrame):\n",
        "    raise NameError(\"The `results_df` DataFrame is not defined. Please run the experimental suite (Cell 5) before this one.\")\n",
        "else:\n",
        "    print(\"✅ Prerequisite check passed: `results_df` is available for analysis.\")\n",
        "\n",
        "# ----------------------------------------------------------------------------\n",
        "# 3. DATA AGGREGATION ACROSS SEEDS\n",
        "# ----------------------------------------------------------------------------\n",
        "print(\"\\n--- Aggregating results across random seeds ---\")\n",
        "\n",
        "# Group by model type and test noise level, then calculate mean and std deviation.\n",
        "summary_df = results_df.groupby(['model', 'test_noise']).agg(\n",
        "    test_accuracy_mean=('test_accuracy', 'mean'),\n",
        "    test_accuracy_std=('test_accuracy', 'std')\n",
        ").reset_index()\n",
        "\n",
        "# Save the aggregated summary for easy access\n",
        "summary_csv_path = PROJECT_CONFIG[\"run_dir\"] / \"phase1b_summary_results.csv\"\n",
        "summary_df.to_csv(summary_csv_path, index=False)\n",
        "\n",
        "print(\"\\n--- Aggregated Results Summary (Mean ± Std) ---\")\n",
        "print(summary_df.to_string())\n",
        "print(f\"\\n✅ Aggregated summary saved to: {summary_csv_path}\")\n",
        "\n",
        "# ----------------------------------------------------------------------------\n",
        "# 4. GENERATE PUBLICATION-QUALITY VISUALIZATION\n",
        "# ----------------------------------------------------------------------------\n",
        "print(\"\\n--- Generating comparative visualization ---\")\n",
        "\n",
        "# Set publication-quality style\n",
        "sns.set_style(\"whitegrid\")\n",
        "plt.rcParams['figure.figsize'] = (12, 8)\n",
        "plt.rcParams['font.size'] = 12\n",
        "\n",
        "# Create plot\n",
        "fig, ax = plt.subplots()\n",
        "\n",
        "# Define distinct styles for each model for maximum clarity\n",
        "styles = {\n",
        "    'HTL':      {'color': '#0072B2', 'marker': 'o', 'label': 'HTL (Trained Noisy)'},\n",
        "    'Dense':    {'color': '#D55E00', 'marker': 's', 'label': 'Dense (Trained Clean)'},\n",
        "    'DenseAug': {'color': '#009E73', 'marker': '^', 'label': 'DenseAug (Trained Noisy)'}\n",
        "}\n",
        "\n",
        "# Plot each model's performance curve with error bars\n",
        "for model_name, style in styles.items():\n",
        "    model_data = summary_df[summary_df['model'] == model_name]\n",
        "\n",
        "    if not model_data.empty:\n",
        "        ax.errorbar(\n",
        "            x=model_data['test_noise'],\n",
        "            y=model_data['test_accuracy_mean'],\n",
        "            yerr=model_data['test_accuracy_std'],\n",
        "            label=style['label'],\n",
        "            color=style['color'],\n",
        "            marker=style['marker'],\n",
        "            markersize=8,\n",
        "            linewidth=2.5,\n",
        "            capsize=5, # Width of the error bar caps\n",
        "            capthick=2,\n",
        "            alpha=0.9\n",
        "        )\n",
        "\n",
        "# --- Formatting and Annotations ---\n",
        "ax.set_title('Project Janus: Model Robustness vs. Input Noise', fontsize=18, weight='bold', pad=20)\n",
        "ax.set_xlabel('Test Noise Level (ε: Pixel Dropout Probability)', fontsize=14, weight='bold')\n",
        "ax.set_ylabel('Test Accuracy (Mean ± Std over 3 Seeds)', fontsize=14, weight='bold')\n",
        "\n",
        "ax.set_xlim(-0.02, 0.52)\n",
        "ax.set_ylim(0.0, 1.05)\n",
        "ax.grid(True, which='both', linestyle='--', linewidth=0.5)\n",
        "ax.legend(loc='lower left', fontsize=12, frameon=True, shadow=True)\n",
        "\n",
        "# Format y-axis as percentage\n",
        "ax.yaxis.set_major_formatter(plt.FuncFormatter(lambda y, _: f'{y:.0%}'))\n",
        "\n",
        "# Add a shaded region to highlight the high-noise regime\n",
        "ax.axvspan(0.3, 0.5, alpha=0.1, color='red')\n",
        "ax.text(0.4, 0.1, 'High Corruption\\nRegime', ha='center', fontsize=12,\n",
        "        color='darkred', style='italic', alpha=0.8)\n",
        "\n",
        "plt.tight_layout()\n",
        "\n",
        "# --- Save and Display ---\n",
        "plot_path = PROJECT_CONFIG[\"run_dir\"] / \"phase1b_robustness_comparison.png\"\n",
        "plt.savefig(plot_path, dpi=300, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "print(f\"\\n✅ Publication-quality visualization saved to: {plot_path}\")\n",
        "print(\"\\nProceed to the final cell for statistical analysis.\")"
      ],
      "metadata": {
        "id": "trer8lrQL486"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# CELL 7: PHASE 1B - STATISTICAL ANALYSIS\n",
        "# ============================================================================\n",
        "# This cell provides the final, quantitative validation of our results.\n",
        "# It calculates a single robustness metric (Area Under Curve) and performs\n",
        "# statistical tests to determine the significance of the findings.\n",
        "# ============================================================================\n",
        "\n",
        "# ----------------------------------------------------------------------------\n",
        "# 1. IMPORT LIBRARIES\n",
        "# ----------------------------------------------------------------------------\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from scipy import stats\n",
        "\n",
        "# ----------------------------------------------------------------------------\n",
        "# 2. PREREQUISITE CHECK\n",
        "# ----------------------------------------------------------------------------\n",
        "# Ensure the raw results from Cell 5 are available.\n",
        "if 'results_df' not in locals() or not isinstance(results_df, pd.DataFrame):\n",
        "    raise NameError(\"The `results_df` DataFrame is not defined. Please run the experimental suite (Cell 5) before this one.\")\n",
        "else:\n",
        "    print(\"✅ Prerequisite check passed: `results_df` is available for analysis.\")\n",
        "\n",
        "# ----------------------------------------------------------------------------\n",
        "# 3. CALCULATE ROBUSTNESS METRIC: AREA UNDER CURVE (AUC)\n",
        "# ----------------------------------------------------------------------------\n",
        "print(\"\\n--- Calculating Robustness Metric: Area Under the Accuracy Curve (AUC) ---\")\n",
        "print(\"This metric integrates accuracy over all tested noise levels. Higher is better.\")\n",
        "\n",
        "auc_results = []\n",
        "\n",
        "# Iterate over every unique model and seed combination\n",
        "for model_name in results_df['model'].unique():\n",
        "    for seed in results_df['seed'].unique():\n",
        "        # Filter the DataFrame for the specific run\n",
        "        subset = results_df[\n",
        "            (results_df['model'] == model_name) &\n",
        "            (results_df['seed'] == seed)\n",
        "        ].sort_values('test_noise') # IMPORTANT: Sort by x-axis value\n",
        "\n",
        "        if not subset.empty:\n",
        "            # Calculate the area under the curve using the trapezoidal rule\n",
        "            auc = np.trapz(y=subset['test_accuracy'], x=subset['test_noise'])\n",
        "\n",
        "            auc_results.append({\n",
        "                'model': model_name,\n",
        "                'seed': seed,\n",
        "                'auc': auc\n",
        "            })\n",
        "\n",
        "auc_df = pd.DataFrame(auc_results)\n",
        "\n",
        "# Display the aggregated AUC results\n",
        "print(\"\\n--- AUC Summary (Mean ± Std) ---\")\n",
        "print(auc_df.groupby('model')['auc'].agg(['mean', 'std']).to_string())\n",
        "\n",
        "# ----------------------------------------------------------------------------\n",
        "# 4. PERFORM PAIRWISE STATISTICAL HYPOTHESIS TESTS\n",
        "# ----------------------------------------------------------------------------\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"STATISTICAL HYPOTHESIS TESTING\")\n",
        "print(\"=\" * 80)\n",
        "print(\"Performing one-sided Mann-Whitney U tests to check if the first model's\")\n",
        "print(\"AUC is significantly GREATER than the second's (α = 0.05).\")\n",
        "\n",
        "# Extract the AUC scores for each model group\n",
        "htl_auc = auc_df[auc_df['model'] == 'HTL']['auc']\n",
        "dense_auc = auc_df[auc_df['model'] == 'Dense']['auc']\n",
        "denseaug_auc = auc_df[auc_df['model'] == 'DenseAug']['auc']\n",
        "\n",
        "# --- Comparison 1: DenseAug vs. Dense (Sanity Check) ---\n",
        "print(\"\\n--- Test 1: Is noise augmentation effective? (DenseAug vs. Dense) ---\")\n",
        "stat, p_value = stats.mannwhitneyu(denseaug_auc, dense_auc, alternative='greater')\n",
        "print(f\"  - U-statistic: {stat:.3f}, p-value: {p_value:.4f}\")\n",
        "if p_value < 0.05:\n",
        "    print(\"  - ✅ Result is SIGNIFICANT. Noise augmentation provides a clear robustness benefit.\")\n",
        "else:\n",
        "    print(\"  - ✗ Result is NOT SIGNIFICANT. Noise augmentation did not provide a clear benefit.\")\n",
        "\n",
        "# --- Comparison 2: HTL vs. Dense ---\n",
        "print(\"\\n--- Test 2: Is the HTL architecture better than a naive baseline? (HTL vs. Dense) ---\")\n",
        "stat, p_value = stats.mannwhitneyu(htl_auc, dense_auc, alternative='greater')\n",
        "print(f\"  - U-statistic: {stat:.3f}, p-value: {p_value:.4f}\")\n",
        "if p_value < 0.05:\n",
        "    print(\"  - ✅ Result is SIGNIFICANT. The HTL is demonstrably more robust than the standard Dense model.\")\n",
        "else:\n",
        "    print(\"  - ✗ Result is NOT SIGNIFICANT. The HTL did not show a clear advantage over the standard Dense model.\")\n",
        "\n",
        "# --- Comparison 3: HTL vs. DenseAug (The Critical Test) ---\n",
        "print(\"\\n--- Test 3 (CRITICAL): Is the HTL architecture's benefit unique? (HTL vs. DenseAug) ---\")\n",
        "print(\"This tests if the physics-informed structure provides an advantage BEYOND what data augmentation alone can achieve.\")\n",
        "stat, p_value = stats.mannwhitneyu(htl_auc, denseaug_auc, alternative='greater')\n",
        "print(f\"  - U-statistic: {stat:.3f}, p-value: {p_value:.4f}\")\n",
        "if p_value < 0.05:\n",
        "    print(\"  - ✅ Result is SIGNIFICANT. The HTL's architectural advantage is real and not just an effect of training on noisy data.\")\n",
        "    print(\"  - This is a Tier 1 (Strong) result for Project Janus.\")\n",
        "else:\n",
        "    print(\"  - ✗ Result is NOT SIGNIFICANT. The HTL's performance is statistically indistinguishable from a standard model trained with noise augmentation.\")\n",
        "    print(\"  - This is a Tier 2 (Moderate) or Tier 3 result.\")\n",
        "\n",
        "# ----------------------------------------------------------------------------\n",
        "# 5. PROJECT CONCLUSION\n",
        "# ----------------------------------------------------------------------------\n",
        "print(\"\\n\" + \"*\" * 80)\n",
        "print(\"PROJECT JANUS: PHASE 1B COMPLETE\")\n",
        "print(\"All experiments, visualizations, and statistical analyses have been executed.\")\n",
        "print(f\"All artifacts for this run are located in: {PROJECT_CONFIG['run_dir']}\")\n",
        "print(\"*\" * 80)"
      ],
      "metadata": {
        "id": "OSYHbFNnL_t6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nWgkZRK1L_4E"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}